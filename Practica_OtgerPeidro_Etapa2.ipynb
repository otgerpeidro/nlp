{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Función de Preprocesado de Texto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean_text: Limpia el texto eliminando signos de puntuación y convirtiendo todo a minúsculas para uniformidad.\n",
    "tokenize_text: Convierte el texto limpio en tokens (palabras individuales) utilizando la tokenización de NLTK.\n",
    "remove_stopwords: Elimina las palabras comunes que suelen ser poco informativas para los modelos de NLP.\n",
    "lemmatize_words: Reduce las palabras a su forma base o lema, lo que ayuda a consolidar diferentes formas de una palabra para que sean tratadas como una sola entidad.\n",
    "preprocess_text: Integra todas las funciones anteriores en una secuencia de operaciones que prepara el texto completamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/otgerpeidro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/otgerpeidro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/otgerpeidro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpiar el texto removiendo puntuación y convirtiendo el texto a minúsculas.\"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar puntuación\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizar el texto en palabras individuales.\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Eliminar stopwords del texto tokenizado.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    \"\"\"Aplicar lematización a los tokens para reducirlos a su forma base o de lema.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Función completa de preprocesamiento que integra todos los pasos.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    lemmatized_tokens = lemmatize_words(tokens)\n",
    "    # Unir los tokens en una cadena para análisis posterior\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/otgerpeidro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/otgerpeidro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/otgerpeidro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descargar los Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos de nuevo los datasets ya que estamos en otro notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "download_file(\"https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Software.jsonl.gz\", \"Software.jsonl.gz\")\n",
    "download_file(\"https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Digital_Music.jsonl.gz\", \"Digital_Music.jsonl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funciones para Cargar y Preprocesar Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(file_name, n_rows=1000):\n",
    "    data = []\n",
    "    with gzip.open(file_name, 'rt', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n_rows:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "software_data = load_data('Software.jsonl.gz', 1000)\n",
    "digital_music_data = load_data('Digital_Music.jsonl.gz', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el preprocesamiento\n",
    "software_data['preprocessed_text'] = software_data['text'].apply(preprocess_text)\n",
    "digital_music_data['preprocessed_text'] = digital_music_data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                                 mcaffee IS malware   \n",
      "1  I love playing tapped out because it is fun to...   \n",
      "2  I love this flashlight app!  It really illumin...   \n",
      "3                           One of my favorite games   \n",
      "4  Cute game. I am not that good at it but my kid...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0                                    mcaffee malware  \n",
      "1  love playing tapped fun watch town grow earnin...  \n",
      "2  love flashlight app really illuminates dark co...  \n",
      "3                                  one favorite game  \n",
      "4               cute game good kid love nik wallenda  \n",
      "                                                text  \\\n",
      "0  If i had a dollar for how many times I have pl...   \n",
      "1  awesome sound - cant wait to see them in perso...   \n",
      "2  This is a great cd. Good music and plays well....   \n",
      "3  These are not real German singers, they have a...   \n",
      "4  I first heard this playing in a Nagoya shop an...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  dollar many time played cd many time asked ale...  \n",
      "1  awesome sound cant wait see person always miss...  \n",
      "2  great cd good music play well seller responded...  \n",
      "3  real german singer accent nothing advertised m...  \n",
      "4  first heard playing nagoya shop fell love remi...  \n"
     ]
    }
   ],
   "source": [
    "# Mostrar algunos resultados\n",
    "print(software_data[['text', 'preprocessed_text']].head())\n",
    "print(digital_music_data[['text', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: If i had a dollar for how many times I have played this cd and how many times I have asked Alexa to play it, I would be rich. Love this singer along with the Black Pumas. Finding a lot of new music that I like a lot on amazon. Try new things.\n",
      "Preprocesado: dollar many time played cd many time asked alexa play would rich love singer along black puma finding lot new music like lot amazon try new thing\n",
      "---\n",
      "Original: awesome sound - cant wait to see them in person - always miss them when they are in town !\n",
      "Preprocesado: awesome sound cant wait see person always miss town\n",
      "---\n",
      "Original: This is a great cd. Good music and plays well. Seller responded back very quicky and  received it within 3 days\n",
      "Preprocesado: great cd good music play well seller responded back quicky received within 3 day\n",
      "---\n",
      "Original: These are not real German singers, they have accents. It is nothing what they advertised it. Music stinks.\n",
      "Preprocesado: real german singer accent nothing advertised music stink\n",
      "---\n",
      "Original: I first heard this playing in a Nagoya shop and fell in love with the remix of Ke$ha's \"Tik Tok\" (Ke$ha and Lady Gaga are EVERYWHERE in Japan), which then morphed into several other recent pop hits. When the salesgirl handed me the CD, I was pleased to see that it actually included thirty pop songs by the likes of Lady Gaga, Usher, La Roux, Katy Perry, Black-Eyed Peas, Ke$ha, etc. I researched more after I got home, and discovered that this Japanese label (Manhattan Records) exists solely to put out rerecorded and remixed dance CDs based on recent pop hits, the key word being rerecorded. I guessed as much as I was listening to the CD in-store; several of the soundalikes do a great job, others are just so-so. It's great for workouts, though, since the tracks fade one into another with no downtime (the bad side is that if you've got in in shuffle, the beginning of the next song, which is often in a different key, is kind of an awkward fade-out when not played in sequence).\n",
      "Preprocesado: first heard playing nagoya shop fell love remix kehas tik tok keha lady gaga everywhere japan morphed several recent pop hit salesgirl handed cd pleased see actually included thirty pop song like lady gaga usher la roux katy perry blackeyed pea keha etc researched got home discovered japanese label manhattan record exists solely put rerecorded remixed dance cd based recent pop hit key word rerecorded guessed much listening cd instore several soundalikes great job others soso great workout though since track fade one another downtime bad side youve got shuffle beginning next song often different key kind awkward fadeout played sequence\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Visualizar los cambios\n",
    "for index, row in digital_music_data.head(5).iterrows():\n",
    "    print(\"Original:\", row['text'])\n",
    "    print(\"Preprocesado:\", row['preprocessed_text'])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario original: 13721\n",
      "Vocabulario después del preprocesamiento: 8240\n"
     ]
    }
   ],
   "source": [
    "# Calcular tamaño del vocabulario\n",
    "from collections import Counter\n",
    "\n",
    "def vocab_size(data):\n",
    "    all_words = ' '.join(data).split()\n",
    "    vocabulary = Counter(all_words)\n",
    "    return len(vocabulary)\n",
    "\n",
    "original_vocab_size = vocab_size(digital_music_data['text'])\n",
    "processed_vocab_size = vocab_size(digital_music_data['preprocessed_text'])\n",
    "print(\"Vocabulario original:\", original_vocab_size)\n",
    "print(\"Vocabulario después del preprocesamiento:\", processed_vocab_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
